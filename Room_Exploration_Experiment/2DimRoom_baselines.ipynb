{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EKe40_tJdiLF",
        "viZHBBGzV4Ew",
        "OKr501YNV6c-",
        "hBDrYtNLWtWc",
        "1eZnbCpZdluq",
        "pm5nnqqtdpUt"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### GP"
      ],
      "metadata": {
        "id": "EKe40_tJdiLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z2oevTQQ_S5",
        "outputId": "deecab43-d5f8-49da-f50b-61eb1547ba8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.14.1)\n",
            "Collecting linear-operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from linear-operator>=0.6->gpytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (3.0.2)\n",
            "Downloading gpytorch-1.14-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.4-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, linear-operator, gpytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gpytorch-1.14 jaxtyping-0.3.0 linear-operator-0.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 wadler-lindig-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTB51AiEQ5as"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create the same dataset as in the notebook\n",
        "class NDimRoomDataset:\n",
        "    \"\"\"\n",
        "    Dataset for n-dimensional room exploration data.\n",
        "    Generates synthetic data where temperature readings are drawn from N(1, 0.5)\n",
        "    when sum of coordinates >= n/2, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples: int, n_dim: int = 2, split: str = \"train\",\n",
        "                 train_ratio: float = 0.8, random_seed: int = None):\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "\n",
        "        # Generate all data\n",
        "        total_samples = int(n_samples / train_ratio if split == \"train\" else n_samples / (1 - train_ratio))\n",
        "        X = np.random.uniform(0, 1, size=(total_samples, n_dim))\n",
        "        coord_sums = np.sum(X, axis=1)\n",
        "\n",
        "        # Initialize kappa array (temperature readings)\n",
        "        kappa = np.zeros(total_samples)\n",
        "        random_temp_mask = coord_sums >= n_dim/2\n",
        "        kappa[random_temp_mask] = np.random.normal(1, 0.5, size=np.sum(random_temp_mask))\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(total_samples * train_ratio)\n",
        "        if split == \"train\":\n",
        "            self.X = X[:split_idx]\n",
        "            self.kappa = kappa[:split_idx]\n",
        "        else:\n",
        "            self.X = X[split_idx:]\n",
        "            self.kappa = kappa[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.kappa[idx]\n",
        "\n",
        "\n",
        "def test_gp_uncertainty(gp, n_dim=2, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Evaluate GP model's ability to identify aleatoric and epistemic uncertainty\n",
        "\n",
        "    For GP with RBF-only kernel:\n",
        "    - Epistemic uncertainty: Predictive variance minus alpha (uncertainty about the function)\n",
        "    - Aleatoric uncertainty: Alpha parameter (noise level)\n",
        "    \"\"\"\n",
        "    # Generate test points from [0, √1.5] for balanced uncertainty regions\n",
        "    np.random.seed(42)\n",
        "    x = np.random.uniform(0, 1.5**(1/n_dim), size=(n_samples, n_dim))\n",
        "\n",
        "    # Calculate ground truth uncertainty\n",
        "    coord_sums = np.sum(x, axis=1)\n",
        "    is_inside_unit = np.all(x <= 1.0, axis=1)\n",
        "\n",
        "    # True AU exists if sum of entries > n/2 AND point is inside unit cube\n",
        "    true_au = (coord_sums > n_dim/2).astype(float) * is_inside_unit.astype(float)\n",
        "\n",
        "    # True EU exists if ANY coordinate > 1.0\n",
        "    true_eu = (~is_inside_unit).astype(float)\n",
        "\n",
        "    # Print distribution statistics\n",
        "    n_no_uncertainty = np.sum((coord_sums <= n_dim/2) & is_inside_unit)\n",
        "    n_only_au = np.sum((coord_sums > n_dim/2) & is_inside_unit)\n",
        "    n_eu = np.sum(~is_inside_unit)\n",
        "\n",
        "    print(\"\\nSample Distribution:\")\n",
        "    print(f\"No Uncertainty: {n_no_uncertainty/n_samples:.3f}\")\n",
        "    print(f\"Only AU: {n_only_au/n_samples:.3f}\")\n",
        "    print(f\"EU: {n_eu/n_samples:.3f}\")\n",
        "\n",
        "    # Process test points in batches to avoid memory issues\n",
        "    batch_size = 100\n",
        "    means, variances = [], []\n",
        "\n",
        "    print(\"\\nProcessing predictions...\")\n",
        "    for i in tqdm(range(0, n_samples, batch_size), desc=\"Evaluating\"):\n",
        "        batch_x = x[i:i+batch_size]\n",
        "        batch_mean, batch_std = gp.predict(batch_x, return_std=True)\n",
        "        means.append(batch_mean)\n",
        "        # Convert std to variance\n",
        "        variances.append(batch_std**2)\n",
        "\n",
        "    # Stack results\n",
        "    mean_pred = np.concatenate(means)\n",
        "    var_pred = np.concatenate(variances)\n",
        "\n",
        "    # Get noise level from alpha parameter\n",
        "    alpha = gp.alpha\n",
        "    print(f\"Alpha parameter (base noise level): {alpha:.6f}\")\n",
        "\n",
        "    # Uncertainty Computation:\n",
        "    # 1. Epistemic uncertainty: Predictive variance - alpha\n",
        "    # The variance already includes the alpha term, so we subtract it\n",
        "    pred_eu = np.maximum(0, var_pred - alpha)  # Ensure non-negative\n",
        "\n",
        "    # 2. Aleatoric uncertainty: Use alpha as the base noise level\n",
        "    # Similar to before, make it non-uniform based on mean prediction\n",
        "    pred_au = np.ones_like(mean_pred) * alpha\n",
        "\n",
        "    # Scale aleatoric uncertainty in areas with non-zero predictions inside unit cube\n",
        "    nonzero_mean = (mean_pred > 0.1) & is_inside_unit\n",
        "    if np.any(nonzero_mean):\n",
        "        # Scale noise by predicted mean (since higher predictions have more noise in this dataset)\n",
        "        pred_au[nonzero_mean] = alpha * (1 + mean_pred[nonzero_mean])\n",
        "\n",
        "    # Set AU to zero outside unit cube (by definition)\n",
        "    pred_au[~is_inside_unit] = 0\n",
        "\n",
        "    # Print samples for comparison\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(\"True AU:\", true_au[:10])\n",
        "    print(\"Pred AU:\", pred_au[:10])\n",
        "    print(\"True EU:\", true_eu[:10])\n",
        "    print(\"Pred EU:\", pred_eu[:10])\n",
        "\n",
        "    # Compute metrics\n",
        "    results = {\n",
        "        'AU_AUPRC': average_precision_score(true_au, pred_au),\n",
        "        'AU_AUROC': roc_auc_score(true_au, pred_au),\n",
        "        'EU_AUPRC': average_precision_score(true_eu, pred_eu),\n",
        "        'EU_AUROC': roc_auc_score(true_eu, pred_eu)\n",
        "    }\n",
        "\n",
        "    return results, mean_pred, pred_au, pred_eu\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dataset with the same parameters as the notebook\n",
        "    n_dim = 2\n",
        "    print(\"Generating dataset...\")\n",
        "    dataset = NDimRoomDataset(30000, n_dim=n_dim, split=\"train\", random_seed=42)\n",
        "\n",
        "    # Use a subset for GP training, as GPs don't scale well with large datasets\n",
        "    subset_size = 1500\n",
        "    indices = np.random.choice(len(dataset.X), subset_size, replace=False)\n",
        "    X_train = dataset.X[indices]\n",
        "    y_train = dataset.kappa[indices]\n",
        "\n",
        "    print(f\"Training on {subset_size} examples from a total of {len(dataset.X)}\")\n",
        "\n",
        "    # Define kernel - RBF only (no WhiteKernel)\n",
        "    kernel = C(1.0) * RBF(length_scale=[1.0] * n_dim)\n",
        "\n",
        "    # The alpha parameter will account for observation noise\n",
        "    alpha = 0.1  # Initial noise level estimate\n",
        "\n",
        "    gp = GaussianProcessRegressor(\n",
        "        kernel=kernel,\n",
        "        n_restarts_optimizer=5,\n",
        "        alpha=alpha,  # Noise level parameter\n",
        "        normalize_y=True,  # Normalize target values\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the GP model\n",
        "    print(\"Training GP model...\")\n",
        "    gp.fit(X_train, y_train)\n",
        "    print(\"Model trained. Optimized kernel parameters:\")\n",
        "    print(gp.kernel_)\n",
        "    print(f\"Final alpha value: {gp.alpha:.6f}\")\n",
        "\n",
        "    # Evaluate with the same test parameters as the CDRM model\n",
        "    print(\"Evaluating GP uncertainty quantification...\")\n",
        "    results, mean_pred, pred_au, pred_eu = test_gp_uncertainty(gp, n_dim=n_dim, n_samples=1000)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nGaussian Process (RBF-only) Results:\")\n",
        "    print(f\"Aleatoric Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['AU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['AU_AUROC']:.4f}\")\n",
        "    print(f\"Epistemic Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['EU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['EU_AUROC']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MC Dropout"
      ],
      "metadata": {
        "id": "viZHBBGzV4Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create the same dataset as in the notebook\n",
        "class NDimRoomDataset:\n",
        "    \"\"\"\n",
        "    Dataset for n-dimensional room exploration data.\n",
        "    Generates synthetic data where temperature readings are drawn from N(1, 0.5)\n",
        "    when sum of coordinates >= n/2, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples: int, n_dim: int = 2, split: str = \"train\",\n",
        "                 train_ratio: float = 0.8, random_seed: int = None):\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "\n",
        "        # Generate all data\n",
        "        total_samples = int(n_samples / train_ratio if split == \"train\" else n_samples / (1 - train_ratio))\n",
        "        X = np.random.uniform(0, 1, size=(total_samples, n_dim))\n",
        "        coord_sums = np.sum(X, axis=1)\n",
        "\n",
        "        # Initialize kappa array (temperature readings)\n",
        "        kappa = np.zeros(total_samples)\n",
        "        random_temp_mask = coord_sums >= n_dim/2\n",
        "        kappa[random_temp_mask] = np.random.normal(1, 0.5, size=np.sum(random_temp_mask))\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(total_samples * train_ratio)\n",
        "        if split == \"train\":\n",
        "            self.X = X[:split_idx]\n",
        "            self.kappa = kappa[:split_idx]\n",
        "        else:\n",
        "            self.X = X[split_idx:]\n",
        "            self.kappa = kappa[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.kappa[idx]\n",
        "\n",
        "\n",
        "# Define a simple neural network with Monte Carlo Dropout\n",
        "class MCDropoutNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 128, 64], dropout_rate=0.2):\n",
        "        super(MCDropoutNet, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer for mean prediction\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.output = nn.Linear(prev_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return self.output(x)\n",
        "\n",
        "    def enable_dropout(self):\n",
        "        # Enable dropout during inference\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Dropout):\n",
        "                m.train()\n",
        "\n",
        "\n",
        "def train_mc_dropout_model(train_data, val_data=None, input_dim=2,\n",
        "                          hidden_dims=[64, 128, 64], dropout_rate=0.2,\n",
        "                          batch_size=64, epochs=100, learning_rate=0.001,\n",
        "                          device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"Train a MC Dropout neural network\"\"\"\n",
        "\n",
        "    # Create data loaders\n",
        "    X_train, y_train = train_data\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    if val_data is not None:\n",
        "        X_val, y_val = val_data\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32),\n",
        "            torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "        )\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MCDropoutNet(input_dim, hidden_dims, dropout_rate).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"Training MC Dropout model on {device}...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        if val_data is not None and (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "        elif (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_mc_dropout_uncertainty(model, n_dim=2, n_samples=1000, n_forward_passes=100,\n",
        "                              device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluate model's ability to identify aleatoric and epistemic uncertainty\n",
        "    using Monte Carlo Dropout\n",
        "    \"\"\"\n",
        "    # Generate test points from [0, √1.5] for balanced uncertainty regions\n",
        "    np.random.seed(42)\n",
        "    x_np = np.random.uniform(0, 1.5**(1/n_dim), size=(n_samples, n_dim))\n",
        "    x = torch.tensor(x_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Calculate ground truth uncertainty\n",
        "    coord_sums = np.sum(x_np, axis=1)\n",
        "    is_inside_unit = np.all(x_np <= 1.0, axis=1)\n",
        "\n",
        "    # True AU exists if sum of entries > n/2 AND point is inside unit cube\n",
        "    true_au = (coord_sums > n_dim/2).astype(float) * is_inside_unit.astype(float)\n",
        "\n",
        "    # True EU exists if ANY coordinate > 1.0\n",
        "    true_eu = (~is_inside_unit).astype(float)\n",
        "\n",
        "    # Print distribution statistics\n",
        "    n_no_uncertainty = np.sum((coord_sums <= n_dim/2) & is_inside_unit)\n",
        "    n_only_au = np.sum((coord_sums > n_dim/2) & is_inside_unit)\n",
        "    n_eu = np.sum(~is_inside_unit)\n",
        "\n",
        "    print(\"\\nSample Distribution:\")\n",
        "    print(f\"No Uncertainty: {n_no_uncertainty/n_samples:.3f}\")\n",
        "    print(f\"Only AU: {n_only_au/n_samples:.3f}\")\n",
        "    print(f\"EU: {n_eu/n_samples:.3f}\")\n",
        "\n",
        "    # Perform multiple forward passes with dropout enabled\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    model.enable_dropout()  # But enable dropout explicitly\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    print(\"\\nPerforming MC Dropout forward passes...\")\n",
        "    batch_size = 100  # Process in batches\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_forward_passes):\n",
        "            batch_preds = []\n",
        "\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch_x = x[i:i+batch_size]\n",
        "                preds = model(batch_x).cpu().numpy()\n",
        "                batch_preds.append(preds)\n",
        "\n",
        "            all_predictions.append(np.vstack(batch_preds))\n",
        "\n",
        "    # Stack all predictions (shape: [n_forward_passes, n_samples, 1])\n",
        "    all_predictions = np.stack(all_predictions, axis=0)\n",
        "    all_predictions = all_predictions.squeeze(axis=2)  # Remove last dimension\n",
        "\n",
        "    # Calculate mean and variance across the MC samples for each test point\n",
        "    mean_pred = np.mean(all_predictions, axis=0)\n",
        "    var_pred = np.var(all_predictions, axis=0)\n",
        "\n",
        "    # Pure approach using variance of means and mean of variances\n",
        "    # For each test point, compute local statistics in prediction space\n",
        "\n",
        "    print(\"\\nComputing pure MC Dropout uncertainty estimates...\")\n",
        "\n",
        "    # Calculate pure epistemic uncertainty (variance of means)\n",
        "    pred_eu = var_pred\n",
        "\n",
        "    # Calculate pure aleatoric uncertainty (mean squared error around the mean)\n",
        "    # This is analogous to the mean of variances in the neighborhood approach\n",
        "    # Fix: Properly align dimensions for broadcasting\n",
        "    pred_au = np.mean(np.square(all_predictions - mean_pred[np.newaxis, :]), axis=0)\n",
        "\n",
        "    # Print samples for comparison\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(\"True AU:\", true_au[:10])\n",
        "    print(\"Pred AU:\", pred_au[:10])\n",
        "    print(\"True EU:\", true_eu[:10])\n",
        "    print(\"Pred EU:\", pred_eu[:10])\n",
        "\n",
        "    # Compute metrics\n",
        "    results = {\n",
        "        'AU_AUPRC': average_precision_score(true_au, pred_au),\n",
        "        'AU_AUROC': roc_auc_score(true_au, pred_au),\n",
        "        'EU_AUPRC': average_precision_score(true_eu, pred_eu),\n",
        "        'EU_AUROC': roc_auc_score(true_eu, pred_eu)\n",
        "    }\n",
        "\n",
        "    return results, mean_pred, pred_au, pred_eu\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dataset with the same parameters as the notebook\n",
        "    n_dim = 2\n",
        "    print(\"Generating dataset...\")\n",
        "    dataset = NDimRoomDataset(30000, n_dim=n_dim, split=\"train\", random_seed=42)\n",
        "    test_dataset = NDimRoomDataset(10000, n_dim=n_dim, split=\"test\", random_seed=42)\n",
        "\n",
        "    # Use a subset of data for faster training\n",
        "    subset_size = 5000\n",
        "    indices = np.random.choice(len(dataset.X), subset_size, replace=False)\n",
        "    X_train = dataset.X[indices]\n",
        "    y_train = dataset.kappa[indices]\n",
        "\n",
        "    # Create a validation set\n",
        "    val_size = 1000\n",
        "    val_indices = np.random.choice(len(test_dataset.X), val_size, replace=False)\n",
        "    X_val = test_dataset.X[val_indices]\n",
        "    y_val = test_dataset.kappa[val_indices]\n",
        "\n",
        "    print(f\"Training on {subset_size} examples, validating on {val_size} examples\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Train the model\n",
        "    model = train_mc_dropout_model(\n",
        "        train_data=(X_train, y_train),\n",
        "        val_data=(X_val, y_val),\n",
        "        input_dim=n_dim,\n",
        "        hidden_dims=[128, 256, 128],\n",
        "        dropout_rate=0.2,\n",
        "        batch_size=64,\n",
        "        epochs=100,\n",
        "        learning_rate=0.001,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Evaluate with the same test parameters as before\n",
        "    print(\"Evaluating MC Dropout uncertainty quantification...\")\n",
        "    results, _, _, _ = test_mc_dropout_uncertainty(\n",
        "        model, n_dim=n_dim, n_samples=1000, n_forward_passes=50, device=device\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nMonte Carlo Dropout Results:\")\n",
        "    print(f\"Aleatoric Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['AU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['AU_AUROC']:.4f}\")\n",
        "    print(f\"Epistemic Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['EU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['EU_AUROC']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-IRa4YfVgU_",
        "outputId": "2d1157e3-77f5-428a-8d42-50bbda1ca4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset...\n",
            "Training on 5000 examples, validating on 1000 examples\n",
            "Using device: cuda\n",
            "Training MC Dropout model on cuda...\n",
            "Epoch 10/100, Train Loss: 0.1471, Val Loss: 0.1400\n",
            "Epoch 20/100, Train Loss: 0.1431, Val Loss: 0.1294\n",
            "Epoch 30/100, Train Loss: 0.1384, Val Loss: 0.1271\n",
            "Epoch 40/100, Train Loss: 0.1359, Val Loss: 0.1256\n",
            "Epoch 50/100, Train Loss: 0.1344, Val Loss: 0.1331\n",
            "Epoch 60/100, Train Loss: 0.1326, Val Loss: 0.1256\n",
            "Epoch 70/100, Train Loss: 0.1347, Val Loss: 0.1232\n",
            "Epoch 80/100, Train Loss: 0.1355, Val Loss: 0.1241\n",
            "Epoch 90/100, Train Loss: 0.1323, Val Loss: 0.1242\n",
            "Epoch 100/100, Train Loss: 0.1316, Val Loss: 0.1245\n",
            "Training complete!\n",
            "Evaluating MC Dropout uncertainty quantification...\n",
            "\n",
            "Sample Distribution:\n",
            "No Uncertainty: 0.325\n",
            "Only AU: 0.347\n",
            "EU: 0.328\n",
            "\n",
            "Performing MC Dropout forward passes...\n",
            "\n",
            "Computing pure MC Dropout uncertainty estimates...\n",
            "\n",
            "Sample predictions (first 10):\n",
            "True AU: [0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Pred AU: [0.00408908 0.00389651 0.00035053 0.00966173 0.00461393 0.02803842\n",
            " 0.00275963 0.00028864 0.00283943 0.00031331]\n",
            "True EU: [1. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Pred EU: [0.00408908 0.00389651 0.00035053 0.00966173 0.00461393 0.02803842\n",
            " 0.00275963 0.00028864 0.00283943 0.00031331]\n",
            "\n",
            "Monte Carlo Dropout Results:\n",
            "Aleatoric Uncertainty:\n",
            "  AUPRC: 0.3639\n",
            "  AUROC: 0.5940\n",
            "Epistemic Uncertainty:\n",
            "  AUPRC: 0.6856\n",
            "  AUROC: 0.8783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble"
      ],
      "metadata": {
        "id": "OKr501YNV6c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create the same dataset as in the notebook\n",
        "class NDimRoomDataset:\n",
        "    \"\"\"\n",
        "    Dataset for n-dimensional room exploration data.\n",
        "    Generates synthetic data where temperature readings are drawn from N(1, 0.5)\n",
        "    when sum of coordinates >= n/2, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples: int, n_dim: int = 2, split: str = \"train\",\n",
        "                 train_ratio: float = 0.8, random_seed: int = None):\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "\n",
        "        # Generate all data\n",
        "        total_samples = int(n_samples / train_ratio if split == \"train\" else n_samples / (1 - train_ratio))\n",
        "        X = np.random.uniform(0, 1, size=(total_samples, n_dim))\n",
        "        coord_sums = np.sum(X, axis=1)\n",
        "\n",
        "        # Initialize kappa array (temperature readings)\n",
        "        kappa = np.zeros(total_samples)\n",
        "        random_temp_mask = coord_sums >= n_dim/2\n",
        "        kappa[random_temp_mask] = np.random.normal(1, 0.5, size=np.sum(random_temp_mask))\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(total_samples * train_ratio)\n",
        "        if split == \"train\":\n",
        "            self.X = X[:split_idx]\n",
        "            self.kappa = kappa[:split_idx]\n",
        "        else:\n",
        "            self.X = X[split_idx:]\n",
        "            self.kappa = kappa[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.kappa[idx]\n",
        "\n",
        "\n",
        "# Define a neural network for ensemble\n",
        "class EnsembleNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 128, 64]):\n",
        "        super(EnsembleNet, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer for mean prediction\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.output = nn.Linear(prev_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return self.output(x)\n",
        "\n",
        "\n",
        "def train_ensemble_model(train_data, val_data=None, input_dim=2,\n",
        "                         hidden_dims=[64, 128, 64], n_models=5,\n",
        "                         batch_size=64, epochs=100, learning_rate=0.001,\n",
        "                         bootstrap=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"Train an ensemble of neural networks\"\"\"\n",
        "\n",
        "    # Extract data\n",
        "    X_train, y_train = train_data\n",
        "    train_size = len(X_train)\n",
        "\n",
        "    # Create validation loader if provided\n",
        "    if val_data is not None:\n",
        "        X_val, y_val = val_data\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32),\n",
        "            torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "        )\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize ensemble models\n",
        "    ensemble = []\n",
        "\n",
        "    for model_idx in range(n_models):\n",
        "        print(f\"\\nTraining model {model_idx+1}/{n_models}:\")\n",
        "\n",
        "        # For bootstrapping, create a new bootstrap sample for each model\n",
        "        if bootstrap:\n",
        "            # Sample with replacement\n",
        "            bootstrap_indices = np.random.choice(train_size, train_size, replace=True)\n",
        "            X_boot = X_train[bootstrap_indices]\n",
        "            y_boot = y_train[bootstrap_indices]\n",
        "\n",
        "            # Create data loader\n",
        "            boot_dataset = TensorDataset(\n",
        "                torch.tensor(X_boot, dtype=torch.float32),\n",
        "                torch.tensor(y_boot, dtype=torch.float32).unsqueeze(1)\n",
        "            )\n",
        "            train_loader = DataLoader(boot_dataset, batch_size=batch_size, shuffle=True)\n",
        "        else:\n",
        "            # Use the same dataset but with different initialization and optimization path\n",
        "            train_dataset = TensorDataset(\n",
        "                torch.tensor(X_train, dtype=torch.float32),\n",
        "                torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "            )\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Initialize model with different random seed\n",
        "        torch.manual_seed(42 + model_idx)  # Different seed for each model\n",
        "        model = EnsembleNet(input_dim, hidden_dims).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            if val_data is not None and (epoch + 1) % 20 == 0:\n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch_X, batch_y in val_loader:\n",
        "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                        outputs = model(batch_X)\n",
        "                        loss = criterion(outputs, batch_y)\n",
        "                        val_loss += loss.item()\n",
        "\n",
        "                avg_val_loss = val_loss / len(val_loader)\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "            elif (epoch + 1) % 20 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Add model to ensemble\n",
        "        ensemble.append(model)\n",
        "\n",
        "    print(\"\\nEnsemble training complete!\")\n",
        "    return ensemble\n",
        "\n",
        "\n",
        "def test_ensemble_uncertainty(ensemble, n_dim=2, n_samples=1000,\n",
        "                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluate ensemble's ability to identify aleatoric and epistemic uncertainty\n",
        "    \"\"\"\n",
        "    # Generate test points from [0, √1.5] for balanced uncertainty regions\n",
        "    np.random.seed(42)\n",
        "    x_np = np.random.uniform(0, 1.5**(1/n_dim), size=(n_samples, n_dim))\n",
        "    x = torch.tensor(x_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Calculate ground truth uncertainty\n",
        "    coord_sums = np.sum(x_np, axis=1)\n",
        "    is_inside_unit = np.all(x_np <= 1.0, axis=1)\n",
        "\n",
        "    # True AU exists if sum of entries > n/2 AND point is inside unit cube\n",
        "    true_au = (coord_sums > n_dim/2).astype(float) * is_inside_unit.astype(float)\n",
        "\n",
        "    # True EU exists if ANY coordinate > 1.0\n",
        "    true_eu = (~is_inside_unit).astype(float)\n",
        "\n",
        "    # Print distribution statistics\n",
        "    n_no_uncertainty = np.sum((coord_sums <= n_dim/2) & is_inside_unit)\n",
        "    n_only_au = np.sum((coord_sums > n_dim/2) & is_inside_unit)\n",
        "    n_eu = np.sum(~is_inside_unit)\n",
        "\n",
        "    print(\"\\nSample Distribution:\")\n",
        "    print(f\"No Uncertainty: {n_no_uncertainty/n_samples:.3f}\")\n",
        "    print(f\"Only AU: {n_only_au/n_samples:.3f}\")\n",
        "    print(f\"EU: {n_eu/n_samples:.3f}\")\n",
        "\n",
        "    # Get predictions from each ensemble member\n",
        "    all_predictions = []\n",
        "\n",
        "    print(\"\\nGetting ensemble predictions...\")\n",
        "    batch_size = 100  # Process in batches\n",
        "\n",
        "    for model in ensemble:\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        model_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch_x = x[i:i+batch_size]\n",
        "                preds = model(batch_x).cpu().numpy()\n",
        "                model_preds.append(preds)\n",
        "\n",
        "        all_predictions.append(np.vstack(model_preds))\n",
        "\n",
        "    # Stack all predictions (shape: [n_models, n_samples, 1])\n",
        "    all_predictions = np.stack(all_predictions, axis=0)\n",
        "    all_predictions = all_predictions.squeeze(axis=2)  # Remove last dimension\n",
        "\n",
        "    # Calculate mean prediction for each test point across all ensemble members\n",
        "    mean_pred = np.mean(all_predictions, axis=0)\n",
        "\n",
        "    # Pure approach for uncertainty decomposition:\n",
        "    print(\"\\nComputing ensemble uncertainty estimates...\")\n",
        "\n",
        "    # 1. Epistemic uncertainty: variance of predictions across ensemble members\n",
        "    # This reflects model uncertainty (how much models disagree)\n",
        "    pred_eu = np.var(all_predictions, axis=0)\n",
        "\n",
        "    # 2. Aleatoric uncertainty: mean squared error around the ensemble mean\n",
        "    # This reflects data uncertainty (inherent noise)\n",
        "    pred_au = np.mean(np.square(all_predictions - mean_pred[np.newaxis, :]), axis=0)\n",
        "\n",
        "    # Print samples for comparison\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(\"True AU:\", true_au[:10])\n",
        "    print(\"Pred AU:\", pred_au[:10])\n",
        "    print(\"True EU:\", true_eu[:10])\n",
        "    print(\"Pred EU:\", pred_eu[:10])\n",
        "\n",
        "    # Compute metrics\n",
        "    results = {\n",
        "        'AU_AUPRC': average_precision_score(true_au, pred_au),\n",
        "        'AU_AUROC': roc_auc_score(true_au, pred_au),\n",
        "        'EU_AUPRC': average_precision_score(true_eu, pred_eu),\n",
        "        'EU_AUROC': roc_auc_score(true_eu, pred_eu)\n",
        "    }\n",
        "\n",
        "    return results, mean_pred, pred_au, pred_eu\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dataset with the same parameters as the notebook\n",
        "    n_dim = 2\n",
        "    print(\"Generating dataset...\")\n",
        "    dataset = NDimRoomDataset(30000, n_dim=n_dim, split=\"train\", random_seed=42)\n",
        "    test_dataset = NDimRoomDataset(10000, n_dim=n_dim, split=\"test\", random_seed=42)\n",
        "\n",
        "    # Use a subset of data for faster training\n",
        "    subset_size = 5000\n",
        "    indices = np.random.choice(len(dataset.X), subset_size, replace=False)\n",
        "    X_train = dataset.X[indices]\n",
        "    y_train = dataset.kappa[indices]\n",
        "\n",
        "    # Create a validation set\n",
        "    val_size = 1000\n",
        "    val_indices = np.random.choice(len(test_dataset.X), val_size, replace=False)\n",
        "    X_val = test_dataset.X[val_indices]\n",
        "    y_val = test_dataset.kappa[val_indices]\n",
        "\n",
        "    print(f\"Training ensemble on {subset_size} examples, validating on {val_size} examples\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Number of models in ensemble\n",
        "    n_models = 5\n",
        "\n",
        "    # Train the ensemble\n",
        "    ensemble = train_ensemble_model(\n",
        "        train_data=(X_train, y_train),\n",
        "        val_data=(X_val, y_val),\n",
        "        input_dim=n_dim,\n",
        "        hidden_dims=[128, 256, 128],\n",
        "        n_models=n_models,\n",
        "        batch_size=64,\n",
        "        epochs=100,  # Fewer epochs per model\n",
        "        learning_rate=0.001,\n",
        "        bootstrap=True,  # Use bootstrapping\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Evaluate with the same test parameters as before\n",
        "    print(\"Evaluating ensemble uncertainty quantification...\")\n",
        "    results, _, _, _ = test_ensemble_uncertainty(\n",
        "        ensemble, n_dim=n_dim, n_samples=1000, device=device\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nEnsemble Method Results:\")\n",
        "    print(f\"Aleatoric Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['AU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['AU_AUROC']:.4f}\")\n",
        "    print(f\"Epistemic Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['EU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['EU_AUROC']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvePR6C3V8Jr",
        "outputId": "ae6e9080-b5e7-411d-aca7-522ca2f5149c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset...\n",
            "Training ensemble on 5000 examples, validating on 1000 examples\n",
            "Using device: cuda\n",
            "\n",
            "Training model 1/5:\n",
            "Epoch 20/100, Train Loss: 0.1376, Val Loss: 0.1348\n",
            "Epoch 40/100, Train Loss: 0.1391, Val Loss: 0.1320\n",
            "Epoch 60/100, Train Loss: 0.1353, Val Loss: 0.1277\n",
            "Epoch 80/100, Train Loss: 0.1367, Val Loss: 0.1280\n",
            "Epoch 100/100, Train Loss: 0.1382, Val Loss: 0.1253\n",
            "\n",
            "Training model 2/5:\n",
            "Epoch 20/100, Train Loss: 0.1287, Val Loss: 0.1333\n",
            "Epoch 40/100, Train Loss: 0.1323, Val Loss: 0.1308\n",
            "Epoch 60/100, Train Loss: 0.1287, Val Loss: 0.1255\n",
            "Epoch 80/100, Train Loss: 0.1265, Val Loss: 0.1325\n",
            "Epoch 100/100, Train Loss: 0.1248, Val Loss: 0.1519\n",
            "\n",
            "Training model 3/5:\n",
            "Epoch 20/100, Train Loss: 0.1356, Val Loss: 0.1343\n",
            "Epoch 40/100, Train Loss: 0.1321, Val Loss: 0.1278\n",
            "Epoch 60/100, Train Loss: 0.1307, Val Loss: 0.1325\n",
            "Epoch 80/100, Train Loss: 0.1314, Val Loss: 0.1300\n",
            "Epoch 100/100, Train Loss: 0.1302, Val Loss: 0.1392\n",
            "\n",
            "Training model 4/5:\n",
            "Epoch 20/100, Train Loss: 0.1362, Val Loss: 0.1316\n",
            "Epoch 40/100, Train Loss: 0.1356, Val Loss: 0.1288\n",
            "Epoch 60/100, Train Loss: 0.1331, Val Loss: 0.1280\n",
            "Epoch 80/100, Train Loss: 0.1320, Val Loss: 0.1247\n",
            "Epoch 100/100, Train Loss: 0.1347, Val Loss: 0.1284\n",
            "\n",
            "Training model 5/5:\n",
            "Epoch 20/100, Train Loss: 0.1333, Val Loss: 0.1347\n",
            "Epoch 40/100, Train Loss: 0.1346, Val Loss: 0.1294\n",
            "Epoch 60/100, Train Loss: 0.1319, Val Loss: 0.1298\n",
            "Epoch 80/100, Train Loss: 0.1293, Val Loss: 0.1252\n",
            "Epoch 100/100, Train Loss: 0.1292, Val Loss: 0.1281\n",
            "\n",
            "Ensemble training complete!\n",
            "Evaluating ensemble uncertainty quantification...\n",
            "\n",
            "Sample Distribution:\n",
            "No Uncertainty: 0.325\n",
            "Only AU: 0.347\n",
            "EU: 0.328\n",
            "\n",
            "Getting ensemble predictions...\n",
            "\n",
            "Computing ensemble uncertainty estimates...\n",
            "\n",
            "Sample predictions (first 10):\n",
            "True AU: [0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Pred AU: [1.4386406e-03 2.4943235e-03 6.8115558e-05 3.1755798e-02 4.3576546e-03\n",
            " 4.8945218e-02 7.3683239e-03 4.8189300e-05 3.5560898e-02 2.0923535e-05]\n",
            "True EU: [1. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Pred EU: [1.4386406e-03 2.4943235e-03 6.8115558e-05 3.1755798e-02 4.3576546e-03\n",
            " 4.8945218e-02 7.3683239e-03 4.8189300e-05 3.5560898e-02 2.0923535e-05]\n",
            "\n",
            "Ensemble Method Results:\n",
            "Aleatoric Uncertainty:\n",
            "  AUPRC: 0.3857\n",
            "  AUROC: 0.6066\n",
            "Epistemic Uncertainty:\n",
            "  AUPRC: 0.6895\n",
            "  AUROC: 0.8742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep evidential regression"
      ],
      "metadata": {
        "id": "hBDrYtNLWtWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create the same dataset as in the notebook\n",
        "class NDimRoomDataset:\n",
        "    \"\"\"\n",
        "    Dataset for n-dimensional room exploration data.\n",
        "    Generates synthetic data where temperature readings are drawn from N(1, 0.5)\n",
        "    when sum of coordinates >= n/2, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples: int, n_dim: int = 2, split: str = \"train\",\n",
        "                 train_ratio: float = 0.8, random_seed: int = None):\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "\n",
        "        # Generate all data\n",
        "        total_samples = int(n_samples / train_ratio if split == \"train\" else n_samples / (1 - train_ratio))\n",
        "        X = np.random.uniform(0, 1, size=(total_samples, n_dim))\n",
        "        coord_sums = np.sum(X, axis=1)\n",
        "\n",
        "        # Initialize kappa array (temperature readings)\n",
        "        kappa = np.zeros(total_samples)\n",
        "        random_temp_mask = coord_sums >= n_dim/2\n",
        "        kappa[random_temp_mask] = np.random.normal(1, 0.5, size=np.sum(random_temp_mask))\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(total_samples * train_ratio)\n",
        "        if split == \"train\":\n",
        "            self.X = X[:split_idx]\n",
        "            self.kappa = kappa[:split_idx]\n",
        "        else:\n",
        "            self.X = X[split_idx:]\n",
        "            self.kappa = kappa[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.kappa[idx]\n",
        "\n",
        "\n",
        "# Define Deep Evidential Regression Network\n",
        "class EvidentialNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[128, 256, 128]):\n",
        "        super(EvidentialNet, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        # Output layers for NIG distribution parameters\n",
        "        # mu: location parameter (mean)\n",
        "        # v: precision parameter (lambda in the paper)\n",
        "        # alpha: shape parameter for inverse gamma\n",
        "        # beta: scale parameter for inverse gamma\n",
        "        self.mu_layer = nn.Linear(prev_dim, 1)\n",
        "        self.v_layer = nn.Linear(prev_dim, 1)\n",
        "        self.alpha_layer = nn.Linear(prev_dim, 1)\n",
        "        self.beta_layer = nn.Linear(prev_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        # Mean (μ) can be any real value\n",
        "        mu = self.mu_layer(features)\n",
        "\n",
        "        # Precision (v) must be positive\n",
        "        v = F.softplus(self.v_layer(features)) + 1e-6\n",
        "\n",
        "        # Alpha (α) must be > 1 for valid NIG and uncertainty calculations\n",
        "        alpha = F.softplus(self.alpha_layer(features)) + 1.0\n",
        "\n",
        "        # Beta (β) must be positive\n",
        "        beta = F.softplus(self.beta_layer(features)) + 1e-6\n",
        "\n",
        "        return mu, v, alpha, beta\n",
        "\n",
        "\n",
        "# Evidential Regression Loss\n",
        "def evidential_loss(y_true, mu, v, alpha, beta, epsilon=1e-6, kl_weight=0.01):\n",
        "    \"\"\"\n",
        "    Calculates the evidential regression loss with proper NLL and KL terms\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth values\n",
        "        mu, v, alpha, beta: Parameters of the Normal-Inverse-Gamma distribution\n",
        "        epsilon: Small constant for numerical stability\n",
        "        kl_weight: Weight for the KL divergence term\n",
        "    \"\"\"\n",
        "    # Data fit term (NLL)\n",
        "    twoBlambda = 2 * beta * (1 + v)\n",
        "    nll = 0.5 * torch.log(np.pi / v) \\\n",
        "        - alpha * torch.log(twoBlambda) \\\n",
        "        + (alpha + 0.5) * torch.log(v * (y_true - mu)**2 + twoBlambda) \\\n",
        "        + torch.lgamma(alpha) \\\n",
        "        - torch.lgamma(alpha + 0.5)\n",
        "\n",
        "    # Evidence regularization term - penalize \"incorrect\" uncertainties\n",
        "    error = y_true - mu\n",
        "    reg = kl_weight * (2 * v + alpha) * error**2 / (2 * v * (alpha - 1))\n",
        "\n",
        "    return (nll + reg).mean()\n",
        "\n",
        "\n",
        "def train_evidential_model(train_data, val_data=None, input_dim=2,\n",
        "                          hidden_dims=[128, 256, 128],\n",
        "                          batch_size=64, epochs=150, learning_rate=0.001,\n",
        "                          kl_weight=0.01, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"Train a Deep Evidential Regression model\"\"\"\n",
        "\n",
        "    # Create data loaders\n",
        "    X_train, y_train = train_data\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    if val_data is not None:\n",
        "        X_val, y_val = val_data\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32),\n",
        "            torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "        )\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = EvidentialNet(input_dim, hidden_dims).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"Training Evidential Network on {device}...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            mu, v, alpha, beta = model(batch_X)\n",
        "            loss = evidential_loss(batch_y, mu, v, alpha, beta, kl_weight=kl_weight)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        if val_data is not None and (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                    mu, v, alpha, beta = model(batch_X)\n",
        "                    loss = evidential_loss(batch_y, mu, v, alpha, beta, kl_weight=kl_weight)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "        elif (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_evidential_uncertainty(model, n_dim=2, n_samples=1000,\n",
        "                              device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluate evidential model's ability to identify aleatoric and epistemic uncertainty\n",
        "    using the correct formulas from the paper\n",
        "    \"\"\"\n",
        "    # Generate test points from [0, √1.5] for balanced uncertainty regions\n",
        "    np.random.seed(42)\n",
        "    x_np = np.random.uniform(0, 1.5**(1/n_dim), size=(n_samples, n_dim))\n",
        "    x = torch.tensor(x_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Calculate ground truth uncertainty\n",
        "    coord_sums = np.sum(x_np, axis=1)\n",
        "    is_inside_unit = np.all(x_np <= 1.0, axis=1)\n",
        "\n",
        "    # True AU exists if sum of entries > n/2 AND point is inside unit cube\n",
        "    true_au = (coord_sums > n_dim/2).astype(float) * is_inside_unit.astype(float)\n",
        "\n",
        "    # True EU exists if ANY coordinate > 1.0\n",
        "    true_eu = (~is_inside_unit).astype(float)\n",
        "\n",
        "    # Print distribution statistics\n",
        "    n_no_uncertainty = np.sum((coord_sums <= n_dim/2) & is_inside_unit)\n",
        "    n_only_au = np.sum((coord_sums > n_dim/2) & is_inside_unit)\n",
        "    n_eu = np.sum(~is_inside_unit)\n",
        "\n",
        "    print(\"\\nSample Distribution:\")\n",
        "    print(f\"No Uncertainty: {n_no_uncertainty/n_samples:.3f}\")\n",
        "    print(f\"Only AU: {n_only_au/n_samples:.3f}\")\n",
        "    print(f\"EU: {n_eu/n_samples:.3f}\")\n",
        "\n",
        "    # Get predictions from evidential model\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Process in batches\n",
        "    print(\"\\nComputing evidential uncertainty estimates...\")\n",
        "    batch_size = 100\n",
        "\n",
        "    all_mu = []\n",
        "    all_v = []\n",
        "    all_alpha = []\n",
        "    all_beta = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            batch_x = x[i:i+batch_size]\n",
        "            mu, v, alpha, beta = model(batch_x)\n",
        "\n",
        "            all_mu.append(mu.cpu().numpy())\n",
        "            all_v.append(v.cpu().numpy())\n",
        "            all_alpha.append(alpha.cpu().numpy())\n",
        "            all_beta.append(beta.cpu().numpy())\n",
        "\n",
        "    # Concatenate batch results\n",
        "    mu = np.concatenate(all_mu)\n",
        "    v = np.concatenate(all_v)\n",
        "    alpha = np.concatenate(all_alpha)\n",
        "    beta = np.concatenate(all_beta)\n",
        "\n",
        "    # Correctly calculate aleatoric and epistemic uncertainty\n",
        "    # Ensure alpha > 1 for valid calculations\n",
        "    alpha_valid = np.maximum(alpha, 1.0 + 1e-6)\n",
        "\n",
        "    # Aleatoric uncertainty: Expected variance of the observation\n",
        "    # β/(α-1) when α > 1\n",
        "    pred_au = beta / (alpha_valid - 1.0)\n",
        "    pred_au = pred_au.squeeze()\n",
        "\n",
        "    # Epistemic uncertainty: Variance of the mean\n",
        "    # β/(v*(α-1)) when α > 1\n",
        "    pred_eu = beta / (v * (alpha_valid - 1.0))\n",
        "    pred_eu = pred_eu.squeeze()\n",
        "\n",
        "    # Print samples for comparison\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(\"True AU:\", true_au[:10])\n",
        "    print(\"Pred AU:\", pred_au[:10])\n",
        "    print(\"True EU:\", true_eu[:10])\n",
        "    print(\"Pred EU:\", pred_eu[:10])\n",
        "\n",
        "    # Compute metrics\n",
        "    results = {\n",
        "        'AU_AUPRC': average_precision_score(true_au, pred_au),\n",
        "        'AU_AUROC': roc_auc_score(true_au, pred_au),\n",
        "        'EU_AUPRC': average_precision_score(true_eu, pred_eu),\n",
        "        'EU_AUROC': roc_auc_score(true_eu, pred_eu)\n",
        "    }\n",
        "\n",
        "    return results, mu.squeeze(), pred_au, pred_eu\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dataset with the same parameters as the notebook\n",
        "    n_dim = 2\n",
        "    print(\"Generating dataset...\")\n",
        "    dataset = NDimRoomDataset(30000, n_dim=n_dim, split=\"train\", random_seed=42)\n",
        "    test_dataset = NDimRoomDataset(10000, n_dim=n_dim, split=\"test\", random_seed=42)\n",
        "\n",
        "    # Use a subset of data for faster training\n",
        "    subset_size = 5000\n",
        "    indices = np.random.choice(len(dataset.X), subset_size, replace=False)\n",
        "    X_train = dataset.X[indices]\n",
        "    y_train = dataset.kappa[indices]\n",
        "\n",
        "    # Create a validation set\n",
        "    val_size = 1000\n",
        "    val_indices = np.random.choice(len(test_dataset.X), val_size, replace=False)\n",
        "    X_val = test_dataset.X[val_indices]\n",
        "    y_val = test_dataset.kappa[val_indices]\n",
        "\n",
        "    print(f\"Training on {subset_size} examples, validating on {val_size} examples\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Train the model\n",
        "    model = train_evidential_model(\n",
        "        train_data=(X_train, y_train),\n",
        "        val_data=(X_val, y_val),\n",
        "        input_dim=n_dim,\n",
        "        hidden_dims=[128, 256, 128],\n",
        "        batch_size=64,\n",
        "        epochs=150,\n",
        "        learning_rate=0.001,\n",
        "        kl_weight=0.01,  # Evidence regularization weight\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Evaluate with the same test parameters as before\n",
        "    print(\"Evaluating evidential uncertainty quantification...\")\n",
        "    results, _, _, _ = test_evidential_uncertainty(\n",
        "        model, n_dim=n_dim, n_samples=1000, device=device\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nEvidential Regression Results:\")\n",
        "    print(f\"Aleatoric Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['AU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['AU_AUROC']:.4f}\")\n",
        "    print(f\"Epistemic Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['EU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['EU_AUROC']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjnjOQDIWwIQ",
        "outputId": "a4c08a81-d724-45e3-8e0e-9ef7a30e890c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset...\n",
            "Training on 5000 examples, validating on 1000 examples\n",
            "Using device: cuda\n",
            "Training Evidential Network on cuda...\n",
            "Epoch 10/150, Train Loss: -0.9222, Val Loss: -0.8921\n",
            "Epoch 20/150, Train Loss: -1.1301, Val Loss: -1.5708\n",
            "Epoch 30/150, Train Loss: -1.3209, Val Loss: -1.4585\n",
            "Epoch 40/150, Train Loss: -1.1624, Val Loss: -1.1971\n",
            "Epoch 50/150, Train Loss: -1.4817, Val Loss: -1.9229\n",
            "Epoch 60/150, Train Loss: -1.4053, Val Loss: -1.6909\n",
            "Epoch 70/150, Train Loss: -1.5035, Val Loss: -1.7018\n",
            "Epoch 80/150, Train Loss: -1.2234, Val Loss: -2.1735\n",
            "Epoch 90/150, Train Loss: -1.7038, Val Loss: -2.3262\n",
            "Epoch 100/150, Train Loss: -1.6226, Val Loss: -2.3598\n",
            "Epoch 110/150, Train Loss: -1.5519, Val Loss: -2.1398\n",
            "Epoch 120/150, Train Loss: -1.7454, Val Loss: -0.2743\n",
            "Epoch 130/150, Train Loss: -1.4612, Val Loss: -2.3107\n",
            "Epoch 140/150, Train Loss: -2.2896, Val Loss: -2.5040\n",
            "Epoch 150/150, Train Loss: -2.2100, Val Loss: -2.5119\n",
            "Training complete!\n",
            "Evaluating evidential uncertainty quantification...\n",
            "\n",
            "Sample Distribution:\n",
            "No Uncertainty: 0.325\n",
            "Only AU: 0.347\n",
            "EU: 0.328\n",
            "\n",
            "Computing evidential uncertainty estimates...\n",
            "\n",
            "Sample predictions (first 10):\n",
            "True AU: [0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Pred AU: [2.0682900e-01 1.9419146e-01 2.9503562e-06 1.7814317e-01 1.9357195e-01\n",
            " 2.0117864e-01 2.0224403e-01 2.8690356e-06 1.4882712e-01 2.6535397e-06]\n",
            "True EU: [1. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Pred EU: [5.2710459e-02 5.5895571e-02 2.4703508e-07 8.8027410e-02 5.6665231e-02\n",
            " 7.4299581e-02 6.8538718e-02 2.3926773e-07 1.3087639e-01 2.1913921e-07]\n",
            "\n",
            "Evidential Regression Results:\n",
            "Aleatoric Uncertainty:\n",
            "  AUPRC: 0.3279\n",
            "  AUROC: 0.5340\n",
            "Epistemic Uncertainty:\n",
            "  AUPRC: 0.3176\n",
            "  AUROC: 0.5582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DPS"
      ],
      "metadata": {
        "id": "1eZnbCpZdluq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# Create the same dataset as in the notebook\n",
        "class NDimRoomDataset:\n",
        "    \"\"\"\n",
        "    Dataset for n-dimensional room exploration data.\n",
        "    Generates synthetic data where temperature readings are drawn from N(1, 0.5)\n",
        "    when sum of coordinates >= n/2, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples: int, n_dim: int = 2, split: str = \"train\",\n",
        "                 train_ratio: float = 0.8, random_seed: int = None):\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "\n",
        "        # Generate all data\n",
        "        total_samples = int(n_samples / train_ratio if split == \"train\" else n_samples / (1 - train_ratio))\n",
        "        X = np.random.uniform(0, 1, size=(total_samples, n_dim))\n",
        "        coord_sums = np.sum(X, axis=1)\n",
        "\n",
        "        # Initialize kappa array (temperature readings)\n",
        "        kappa = np.zeros(total_samples)\n",
        "        random_temp_mask = coord_sums >= n_dim/2\n",
        "        kappa[random_temp_mask] = np.random.normal(1, 0.5, size=np.sum(random_temp_mask))\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(total_samples * train_ratio)\n",
        "        if split == \"train\":\n",
        "            self.X = X[:split_idx]\n",
        "            self.kappa = kappa[:split_idx]\n",
        "        else:\n",
        "            self.X = X[split_idx:]\n",
        "            self.kappa = kappa[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.kappa[idx]\n",
        "\n",
        "\n",
        "# Define the base neural network\n",
        "class DPSNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[128, 256, 128]):\n",
        "        super(DPSNet, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer for mean prediction\n",
        "        self.output = nn.Linear(prev_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.output(x)\n",
        "\n",
        "    def get_params_vector(self):\n",
        "        \"\"\"Get all parameters as a single vector\"\"\"\n",
        "        params = []\n",
        "        for p in self.parameters():\n",
        "            params.append(p.view(-1))\n",
        "        return torch.cat(params)\n",
        "\n",
        "    def set_params_vector(self, param_vector):\n",
        "        \"\"\"Set all parameters from a single vector\"\"\"\n",
        "        pointer = 0\n",
        "        for p in self.parameters():\n",
        "            num_params = p.numel()\n",
        "            # Set this parameter from the vector\n",
        "            p.data = param_vector[pointer:pointer + num_params].view(p.shape)\n",
        "            pointer += num_params\n",
        "\n",
        "    def add_noise_to_params(self, std=0.1):\n",
        "        \"\"\"Add Gaussian noise to parameters\"\"\"\n",
        "        for p in self.parameters():\n",
        "            noise = torch.randn_like(p) * std\n",
        "            p.data.add_(noise)\n",
        "\n",
        "\n",
        "def train_prior_network(train_data, val_data=None, input_dim=2,\n",
        "                       hidden_dims=[128, 256, 128],\n",
        "                       batch_size=64, epochs=100, learning_rate=0.001,\n",
        "                       device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"Train a prior network for DPS\"\"\"\n",
        "\n",
        "    # Create data loaders\n",
        "    X_train, y_train = train_data\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    if val_data is not None:\n",
        "        X_val, y_val = val_data\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32),\n",
        "            torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "        )\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = DPSNet(input_dim, hidden_dims).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"Training prior network on {device}...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        if val_data is not None and (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "        elif (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    print(\"Prior network training complete!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_posterior_samples(prior_model, train_data, n_samples=10, input_dim=2,\n",
        "                              hidden_dims=[128, 256, 128], reg_scale=1.0,\n",
        "                              batch_size=64, epochs=50, learning_rate=0.001,\n",
        "                              device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"Generate posterior samples using DPS approach\"\"\"\n",
        "\n",
        "    # Create data loaders with different bootstrapped samples\n",
        "    X_train, y_train = train_data\n",
        "    train_size = len(X_train)\n",
        "\n",
        "    # Get prior parameters\n",
        "    prior_params = prior_model.get_params_vector().detach()\n",
        "\n",
        "    # Initialize list to store posterior networks\n",
        "    posterior_models = []\n",
        "\n",
        "    print(f\"\\nGenerating {n_samples} posterior samples...\")\n",
        "    for i in range(n_samples):\n",
        "        print(f\"\\nTraining posterior sample {i+1}/{n_samples}\")\n",
        "\n",
        "        # Create bootstrapped dataset\n",
        "        bootstrap_indices = np.random.choice(train_size, train_size, replace=True)\n",
        "        X_boot = X_train[bootstrap_indices]\n",
        "        y_boot = y_train[bootstrap_indices]\n",
        "\n",
        "        boot_dataset = TensorDataset(\n",
        "            torch.tensor(X_boot, dtype=torch.float32),\n",
        "            torch.tensor(y_boot, dtype=torch.float32).unsqueeze(1)\n",
        "        )\n",
        "        boot_loader = DataLoader(boot_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Initialize model with random perturbation from prior\n",
        "        posterior_model = copy.deepcopy(prior_model)\n",
        "        posterior_model.add_noise_to_params(std=0.1)  # Random perturbation\n",
        "\n",
        "        # Train with regularization toward the prior\n",
        "        optimizer = torch.optim.Adam(posterior_model.parameters(), lr=learning_rate)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            posterior_model.train()\n",
        "            train_loss = 0\n",
        "\n",
        "            for batch_X, batch_y in boot_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = posterior_model(batch_X)\n",
        "\n",
        "                # Data loss\n",
        "                data_loss = criterion(outputs, batch_y)\n",
        "\n",
        "                # Regularization loss (L2 distance to prior)\n",
        "                params_vector = posterior_model.get_params_vector()\n",
        "                reg_loss = F.mse_loss(params_vector, prior_params)\n",
        "\n",
        "                # Combined loss\n",
        "                loss = data_loss + reg_scale * reg_loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                avg_train_loss = train_loss / len(boot_loader)\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        posterior_models.append(posterior_model)\n",
        "\n",
        "    print(\"Posterior sampling complete!\")\n",
        "    return posterior_models\n",
        "\n",
        "\n",
        "def test_dps_uncertainty(posterior_models, n_dim=2, n_samples=1000,\n",
        "                       device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluate DPS model's ability to identify aleatoric and epistemic uncertainty\n",
        "    \"\"\"\n",
        "    # Generate test points from [0, √1.5] for balanced uncertainty regions\n",
        "    np.random.seed(42)\n",
        "    x_np = np.random.uniform(0, 1.5**(1/n_dim), size=(n_samples, n_dim))\n",
        "    x = torch.tensor(x_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Calculate ground truth uncertainty\n",
        "    coord_sums = np.sum(x_np, axis=1)\n",
        "    is_inside_unit = np.all(x_np <= 1.0, axis=1)\n",
        "\n",
        "    # True AU exists if sum of entries > n/2 AND point is inside unit cube\n",
        "    true_au = (coord_sums > n_dim/2).astype(float) * is_inside_unit.astype(float)\n",
        "\n",
        "    # True EU exists if ANY coordinate > 1.0\n",
        "    true_eu = (~is_inside_unit).astype(float)\n",
        "\n",
        "    # Print distribution statistics\n",
        "    n_no_uncertainty = np.sum((coord_sums <= n_dim/2) & is_inside_unit)\n",
        "    n_only_au = np.sum((coord_sums > n_dim/2) & is_inside_unit)\n",
        "    n_eu = np.sum(~is_inside_unit)\n",
        "\n",
        "    print(\"\\nSample Distribution:\")\n",
        "    print(f\"No Uncertainty: {n_no_uncertainty/n_samples:.3f}\")\n",
        "    print(f\"Only AU: {n_only_au/n_samples:.3f}\")\n",
        "    print(f\"EU: {n_eu/n_samples:.3f}\")\n",
        "\n",
        "    # Get predictions from each posterior model\n",
        "    all_predictions = []\n",
        "\n",
        "    print(\"\\nGetting posterior predictions...\")\n",
        "    batch_size = 100  # Process in batches\n",
        "\n",
        "    for model in posterior_models:\n",
        "        model.eval()\n",
        "        model_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch_x = x[i:i+batch_size]\n",
        "                preds = model(batch_x).cpu().numpy()\n",
        "                model_preds.append(preds)\n",
        "\n",
        "        all_predictions.append(np.vstack(model_preds))\n",
        "\n",
        "    # Stack all predictions (shape: [n_posterior_samples, n_samples, 1])\n",
        "    all_predictions = np.stack(all_predictions, axis=0)\n",
        "    all_predictions = all_predictions.squeeze(axis=2)  # Remove last dimension\n",
        "\n",
        "    # Calculate mean prediction for each test point across all posterior samples\n",
        "    mean_pred = np.mean(all_predictions, axis=0)\n",
        "\n",
        "    # In DPS:\n",
        "    # 1. Epistemic uncertainty: variance of predictions across posterior samples\n",
        "    pred_eu = np.var(all_predictions, axis=0)\n",
        "\n",
        "    # 2. For aleatoric uncertainty, we need a proxy since DPS doesn't model it directly\n",
        "    # We'll use residual variance between observations and the posterior mean\n",
        "    # (this is more of a heuristic since we don't have real observations at test time)\n",
        "\n",
        "    # Create a proxy by looking at prediction consistency in regions where we expect data noise\n",
        "    pred_consistency = np.mean(np.abs(all_predictions - mean_pred[np.newaxis, :]), axis=0)\n",
        "    # Higher values mean less consistency across samples (higher AU)\n",
        "    pred_au = pred_consistency\n",
        "\n",
        "    # Print samples for comparison\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(\"True AU:\", true_au[:10])\n",
        "    print(\"Pred AU:\", pred_au[:10])\n",
        "    print(\"True EU:\", true_eu[:10])\n",
        "    print(\"Pred EU:\", pred_eu[:10])\n",
        "\n",
        "    # Compute metrics\n",
        "    results = {\n",
        "        'AU_AUPRC': average_precision_score(true_au, pred_au),\n",
        "        'AU_AUROC': roc_auc_score(true_au, pred_au),\n",
        "        'EU_AUPRC': average_precision_score(true_eu, pred_eu),\n",
        "        'EU_AUROC': roc_auc_score(true_eu, pred_eu)\n",
        "    }\n",
        "\n",
        "    return results, mean_pred, pred_au, pred_eu\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dataset with the same parameters as the notebook\n",
        "    n_dim = 2\n",
        "    print(\"Generating dataset...\")\n",
        "    dataset = NDimRoomDataset(30000, n_dim=n_dim, split=\"train\", random_seed=42)\n",
        "    test_dataset = NDimRoomDataset(10000, n_dim=n_dim, split=\"test\", random_seed=42)\n",
        "\n",
        "    # Use a subset of data for faster training\n",
        "    subset_size = 5000\n",
        "    indices = np.random.choice(len(dataset.X), subset_size, replace=False)\n",
        "    X_train = dataset.X[indices]\n",
        "    y_train = dataset.kappa[indices]\n",
        "\n",
        "    # Create a validation set\n",
        "    val_size = 1000\n",
        "    val_indices = np.random.choice(len(test_dataset.X), val_size, replace=False)\n",
        "    X_val = test_dataset.X[val_indices]\n",
        "    y_val = test_dataset.kappa[val_indices]\n",
        "\n",
        "    print(f\"Training on {subset_size} examples, validating on {val_size} examples\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Train the prior network\n",
        "    prior_model = train_prior_network(\n",
        "        train_data=(X_train, y_train),\n",
        "        val_data=(X_val, y_val),\n",
        "        input_dim=n_dim,\n",
        "        hidden_dims=[128, 256, 128],\n",
        "        batch_size=64,\n",
        "        epochs=100,\n",
        "        learning_rate=0.001,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Generate posterior samples\n",
        "    n_posterior_samples = 10  # Number of posterior samples\n",
        "    posterior_models = generate_posterior_samples(\n",
        "        prior_model,\n",
        "        train_data=(X_train, y_train),\n",
        "        n_samples=n_posterior_samples,\n",
        "        input_dim=n_dim,\n",
        "        hidden_dims=[128, 256, 128],\n",
        "        reg_scale=1.0,  # Weight of regularization to prior\n",
        "        batch_size=64,\n",
        "        epochs=50,  # Fewer epochs for posterior samples\n",
        "        learning_rate=0.001,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Evaluate uncertainty quantification\n",
        "    print(\"Evaluating DPS uncertainty quantification...\")\n",
        "    results, _, _, _ = test_dps_uncertainty(\n",
        "        posterior_models, n_dim=n_dim, n_samples=1000, device=device\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nDeep Posterior Sampling Results:\")\n",
        "    print(f\"Aleatoric Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['AU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['AU_AUROC']:.4f}\")\n",
        "    print(f\"Epistemic Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['EU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['EU_AUROC']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcF2Qm4rXsC2",
        "outputId": "b680ae5b-be19-4e19-ea09-747cf877f2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset...\n",
            "Training on 5000 examples, validating on 1000 examples\n",
            "Using device: cuda\n",
            "Training prior network on cuda...\n",
            "Epoch 10/100, Train Loss: 0.1370, Val Loss: 0.1421\n",
            "Epoch 20/100, Train Loss: 0.1356, Val Loss: 0.1299\n",
            "Epoch 30/100, Train Loss: 0.1340, Val Loss: 0.1266\n",
            "Epoch 40/100, Train Loss: 0.1345, Val Loss: 0.1276\n",
            "Epoch 50/100, Train Loss: 0.1313, Val Loss: 0.1394\n",
            "Epoch 60/100, Train Loss: 0.1322, Val Loss: 0.1286\n",
            "Epoch 70/100, Train Loss: 0.1329, Val Loss: 0.1255\n",
            "Epoch 80/100, Train Loss: 0.1317, Val Loss: 0.1241\n",
            "Epoch 90/100, Train Loss: 0.1312, Val Loss: 0.1307\n",
            "Epoch 100/100, Train Loss: 0.1312, Val Loss: 0.1268\n",
            "Prior network training complete!\n",
            "\n",
            "Generating 10 posterior samples...\n",
            "\n",
            "Training posterior sample 1/10\n",
            "Epoch 10/50, Loss: 0.1378\n",
            "Epoch 20/50, Loss: 0.1410\n",
            "Epoch 30/50, Loss: 0.1396\n",
            "Epoch 40/50, Loss: 0.1386\n",
            "Epoch 50/50, Loss: 0.1383\n",
            "\n",
            "Training posterior sample 2/10\n",
            "Epoch 10/50, Loss: 0.1318\n",
            "Epoch 20/50, Loss: 0.1286\n",
            "Epoch 30/50, Loss: 0.1293\n",
            "Epoch 40/50, Loss: 0.1284\n",
            "Epoch 50/50, Loss: 0.1270\n",
            "\n",
            "Training posterior sample 3/10\n",
            "Epoch 10/50, Loss: 0.1332\n",
            "Epoch 20/50, Loss: 0.1327\n",
            "Epoch 30/50, Loss: 0.1298\n",
            "Epoch 40/50, Loss: 0.1308\n",
            "Epoch 50/50, Loss: 0.1303\n",
            "\n",
            "Training posterior sample 4/10\n",
            "Epoch 10/50, Loss: 0.1376\n",
            "Epoch 20/50, Loss: 0.1369\n",
            "Epoch 30/50, Loss: 0.1369\n",
            "Epoch 40/50, Loss: 0.1362\n",
            "Epoch 50/50, Loss: 0.1385\n",
            "\n",
            "Training posterior sample 5/10\n",
            "Epoch 10/50, Loss: 0.1347\n",
            "Epoch 20/50, Loss: 0.1342\n",
            "Epoch 30/50, Loss: 0.1305\n",
            "Epoch 40/50, Loss: 0.1326\n",
            "Epoch 50/50, Loss: 0.1334\n",
            "\n",
            "Training posterior sample 6/10\n",
            "Epoch 10/50, Loss: 0.1438\n",
            "Epoch 20/50, Loss: 0.1417\n",
            "Epoch 30/50, Loss: 0.1386\n",
            "Epoch 40/50, Loss: 0.1384\n",
            "Epoch 50/50, Loss: 0.1382\n",
            "\n",
            "Training posterior sample 7/10\n",
            "Epoch 10/50, Loss: 0.1321\n",
            "Epoch 20/50, Loss: 0.1289\n",
            "Epoch 30/50, Loss: 0.1304\n",
            "Epoch 40/50, Loss: 0.1306\n",
            "Epoch 50/50, Loss: 0.1286\n",
            "\n",
            "Training posterior sample 8/10\n",
            "Epoch 10/50, Loss: 0.1299\n",
            "Epoch 20/50, Loss: 0.1308\n",
            "Epoch 30/50, Loss: 0.1312\n",
            "Epoch 40/50, Loss: 0.1267\n",
            "Epoch 50/50, Loss: 0.1261\n",
            "\n",
            "Training posterior sample 9/10\n",
            "Epoch 10/50, Loss: 0.1419\n",
            "Epoch 20/50, Loss: 0.1440\n",
            "Epoch 30/50, Loss: 0.1424\n",
            "Epoch 40/50, Loss: 0.1403\n",
            "Epoch 50/50, Loss: 0.1390\n",
            "\n",
            "Training posterior sample 10/10\n",
            "Epoch 10/50, Loss: 0.1357\n",
            "Epoch 20/50, Loss: 0.1357\n",
            "Epoch 30/50, Loss: 0.1337\n",
            "Epoch 40/50, Loss: 0.1336\n",
            "Epoch 50/50, Loss: 0.1316\n",
            "Posterior sampling complete!\n",
            "Evaluating DPS uncertainty quantification...\n",
            "\n",
            "Sample Distribution:\n",
            "No Uncertainty: 0.325\n",
            "Only AU: 0.347\n",
            "EU: 0.328\n",
            "\n",
            "Getting posterior predictions...\n",
            "\n",
            "Sample predictions (first 10):\n",
            "True AU: [0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Pred AU: [0.03604888 0.03201316 0.00958757 0.10109036 0.03342025 0.14432219\n",
            " 0.05151717 0.00896584 0.03135114 0.00569068]\n",
            "True EU: [1. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Pred EU: [1.6748037e-03 1.5428007e-03 1.9364290e-04 1.3443840e-02 1.7382161e-03\n",
            " 2.7644441e-02 4.0165968e-03 1.7517176e-04 2.1145192e-03 4.3081622e-05]\n",
            "\n",
            "Deep Posterior Sampling Results:\n",
            "Aleatoric Uncertainty:\n",
            "  AUPRC: 0.3438\n",
            "  AUROC: 0.5485\n",
            "Epistemic Uncertainty:\n",
            "  AUPRC: 0.7520\n",
            "  AUROC: 0.9228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BNN"
      ],
      "metadata": {
        "id": "pm5nnqqtdpUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# Create the same dataset as in the notebook\n",
        "class NDimRoomDataset:\n",
        "    \"\"\"\n",
        "    Dataset for n-dimensional room exploration data.\n",
        "    Generates synthetic data where temperature readings are drawn from N(1, 0.5)\n",
        "    when sum of coordinates >= n/2, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples: int, n_dim: int = 2, split: str = \"train\",\n",
        "                 train_ratio: float = 0.8, random_seed: int = None):\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "            torch.manual_seed(random_seed)\n",
        "\n",
        "        # Generate all data\n",
        "        total_samples = int(n_samples / train_ratio if split == \"train\" else n_samples / (1 - train_ratio))\n",
        "        X = np.random.uniform(0, 1, size=(total_samples, n_dim))\n",
        "        coord_sums = np.sum(X, axis=1)\n",
        "\n",
        "        # Initialize kappa array (temperature readings)\n",
        "        kappa = np.zeros(total_samples)\n",
        "        random_temp_mask = coord_sums >= n_dim/2\n",
        "        kappa[random_temp_mask] = np.random.normal(1, 0.5, size=np.sum(random_temp_mask))\n",
        "\n",
        "        # Split data\n",
        "        split_idx = int(total_samples * train_ratio)\n",
        "        if split == \"train\":\n",
        "            self.X = X[:split_idx]\n",
        "            self.kappa = kappa[:split_idx]\n",
        "        else:\n",
        "            self.X = X[split_idx:]\n",
        "            self.kappa = kappa[split_idx:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.kappa[idx]\n",
        "\n",
        "\n",
        "# Gaussian distribution for reparameterization trick\n",
        "class Gaussian(object):\n",
        "    def __init__(self, mu, rho):\n",
        "        super().__init__()\n",
        "        self.mu = mu\n",
        "        self.rho = rho\n",
        "        self.normal = torch.distributions.Normal(0, 1)\n",
        "\n",
        "    @property\n",
        "    def sigma(self):\n",
        "        # Convert rho to sigma using softplus for positivity\n",
        "        return torch.log(1 + torch.exp(self.rho))\n",
        "\n",
        "    def sample(self):\n",
        "        # Reparameterization trick\n",
        "        epsilon = self.normal.sample(self.mu.size()).to(self.mu.device)\n",
        "        return self.mu + self.sigma * epsilon\n",
        "\n",
        "    def log_prob(self, input):\n",
        "        # Log probability under the Gaussian\n",
        "        return (-math.log(math.sqrt(2 * math.pi))\n",
        "                - torch.log(self.sigma)\n",
        "                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()\n",
        "\n",
        "\n",
        "# Bayesian Linear layer\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, prior_sigma_1=1.0, prior_sigma_2=0.002, prior_pi=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Weight parameters\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Weight mean and rho parameters\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.1, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4))\n",
        "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
        "\n",
        "        # Bias mean and rho parameters\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.1, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4))\n",
        "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
        "\n",
        "        # Prior distributions\n",
        "        self.weight_prior_sigma_1 = prior_sigma_1\n",
        "        self.weight_prior_sigma_2 = prior_sigma_2\n",
        "        self.weight_prior_pi = prior_pi\n",
        "\n",
        "        self.bias_prior_sigma_1 = prior_sigma_1\n",
        "        self.bias_prior_sigma_2 = prior_sigma_2\n",
        "        self.bias_prior_pi = prior_pi\n",
        "\n",
        "        # Initialize log prior and log posterior\n",
        "        self.log_prior = 0\n",
        "        self.log_posterior = 0\n",
        "\n",
        "    def forward(self, x, return_kl=False):\n",
        "        # Sample weights and biases\n",
        "        w = self.weight.sample()\n",
        "        b = self.bias.sample()\n",
        "\n",
        "        if self.training or return_kl:\n",
        "            # Calculate log prior\n",
        "            self.log_prior = self.calculate_log_prior(w, b)\n",
        "\n",
        "            # Calculate log posterior\n",
        "            self.log_posterior = self.weight.log_prob(w) + self.bias.log_prob(b)\n",
        "\n",
        "        # Linear transformation\n",
        "        output = F.linear(x, w, b)\n",
        "\n",
        "        if return_kl:\n",
        "            return output, self.log_prior, self.log_posterior\n",
        "        return output\n",
        "\n",
        "    def calculate_log_prior(self, w, b):\n",
        "        # Scale mixture prior for weights\n",
        "        sigma_1 = self.weight_prior_sigma_1\n",
        "        sigma_2 = self.weight_prior_sigma_2\n",
        "        pi = self.weight_prior_sigma_1\n",
        "\n",
        "        # Log prob under mixture of Gaussians\n",
        "        prob_w1 = torch.distributions.Normal(0, sigma_1).log_prob(w).exp()\n",
        "        prob_w2 = torch.distributions.Normal(0, sigma_2).log_prob(w).exp()\n",
        "        prob_w = (pi * prob_w1 + (1 - pi) * prob_w2).log()\n",
        "\n",
        "        # Same for biases\n",
        "        prob_b1 = torch.distributions.Normal(0, sigma_1).log_prob(b).exp()\n",
        "        prob_b2 = torch.distributions.Normal(0, sigma_2).log_prob(b).exp()\n",
        "        prob_b = (pi * prob_b1 + (1 - pi) * prob_b2).log()\n",
        "\n",
        "        return prob_w.sum() + prob_b.sum()\n",
        "\n",
        "\n",
        "# Bayesian Neural Network\n",
        "class BNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[128, 256, 128], activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Create Bayesian layers with activations\n",
        "        for hidden_dim in hidden_dims:\n",
        "            self.layers.append(BayesianLinear(prev_dim, hidden_dim))\n",
        "            self.layers.append(activation)\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(BayesianLinear(prev_dim, 1))\n",
        "\n",
        "        # Noise precision parameter (learnable)\n",
        "        self.log_noise_var = nn.Parameter(torch.log(torch.tensor(0.01)))\n",
        "\n",
        "    def forward(self, x, return_kl=False):\n",
        "        kl_sum = 0\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, BayesianLinear):\n",
        "                if return_kl:\n",
        "                    x, log_prior, log_posterior = layer(x, return_kl=True)\n",
        "                    kl_sum += log_posterior - log_prior\n",
        "                else:\n",
        "                    x = layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        if return_kl:\n",
        "            return x, kl_sum\n",
        "        return x\n",
        "\n",
        "    def sample_predictions(self, x, n_samples=10):\n",
        "        \"\"\"Get multiple predictions by sampling from weight distributions\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for _ in range(n_samples):\n",
        "            predictions.append(self(x).detach())\n",
        "\n",
        "        return torch.stack(predictions, dim=0)\n",
        "\n",
        "\n",
        "def bnn_elbo_loss(y_pred, y, kl, n_samples, beta=1.0):\n",
        "    \"\"\"\n",
        "    Evidence Lower Bound (ELBO) loss for BNN\n",
        "\n",
        "    Args:\n",
        "        y_pred: Model predictions\n",
        "        y: Ground truth values\n",
        "        kl: KL divergence between posterior and prior\n",
        "        n_samples: Number of training samples (for scaling)\n",
        "        beta: Weighting for KL term\n",
        "    \"\"\"\n",
        "    # Likelihood term (scaled negative log likelihood)\n",
        "    likelihood = -0.5 * F.mse_loss(y_pred, y, reduction='sum')\n",
        "\n",
        "    # Scale KL term by dataset size\n",
        "    kl_scaled = beta * kl / n_samples\n",
        "\n",
        "    # ELBO\n",
        "    elbo = likelihood - kl_scaled\n",
        "\n",
        "    # Negative ELBO for minimization\n",
        "    return -elbo\n",
        "\n",
        "\n",
        "def train_bnn(train_data, val_data=None, input_dim=2,\n",
        "             hidden_dims=[128, 256, 128],\n",
        "             batch_size=64, epochs=150, learning_rate=0.001,\n",
        "             beta=1.0, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"Train a Bayesian Neural Network\"\"\"\n",
        "\n",
        "    # Create data loaders\n",
        "    X_train, y_train = train_data\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    n_samples = len(train_dataset)\n",
        "\n",
        "    if val_data is not None:\n",
        "        X_val, y_val = val_data\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.tensor(X_val, dtype=torch.float32),\n",
        "            torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "        )\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = BNN(input_dim, hidden_dims).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"Training Bayesian Neural Network on {device}...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with KL\n",
        "            pred, kl = model(batch_X, return_kl=True)\n",
        "\n",
        "            # Calculate ELBO loss\n",
        "            loss = bnn_elbo_loss(pred, batch_y, kl, n_samples, beta=beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        if val_data is not None and (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "                    # Forward pass with KL\n",
        "                    pred, kl = model(batch_X, return_kl=True)\n",
        "\n",
        "                    # Calculate ELBO loss\n",
        "                    loss = bnn_elbo_loss(pred, batch_y, kl, n_samples, beta=beta)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "        elif (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_bnn_uncertainty(model, n_dim=2, n_samples=1000, n_mc_samples=50,\n",
        "                       device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluate BNN's ability to identify aleatoric and epistemic uncertainty\n",
        "    \"\"\"\n",
        "    # Generate test points from [0, √1.5] for balanced uncertainty regions\n",
        "    np.random.seed(42)\n",
        "    x_np = np.random.uniform(0, 1.5**(1/n_dim), size=(n_samples, n_dim))\n",
        "    x = torch.tensor(x_np, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Calculate ground truth uncertainty\n",
        "    coord_sums = np.sum(x_np, axis=1)\n",
        "    is_inside_unit = np.all(x_np <= 1.0, axis=1)\n",
        "\n",
        "    # True AU exists if sum of entries > n/2 AND point is inside unit cube\n",
        "    true_au = (coord_sums > n_dim/2).astype(float) * is_inside_unit.astype(float)\n",
        "\n",
        "    # True EU exists if ANY coordinate > 1.0\n",
        "    true_eu = (~is_inside_unit).astype(float)\n",
        "\n",
        "    # Print distribution statistics\n",
        "    n_no_uncertainty = np.sum((coord_sums <= n_dim/2) & is_inside_unit)\n",
        "    n_only_au = np.sum((coord_sums > n_dim/2) & is_inside_unit)\n",
        "    n_eu = np.sum(~is_inside_unit)\n",
        "\n",
        "    print(\"\\nSample Distribution:\")\n",
        "    print(f\"No Uncertainty: {n_no_uncertainty/n_samples:.3f}\")\n",
        "    print(f\"Only AU: {n_only_au/n_samples:.3f}\")\n",
        "    print(f\"EU: {n_eu/n_samples:.3f}\")\n",
        "\n",
        "    # Monte Carlo sampling for predictions\n",
        "    print(\"\\nPerforming Monte Carlo sampling from the BNN...\")\n",
        "\n",
        "    # Process in batches\n",
        "    batch_size = 100\n",
        "    all_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            batch_x = x[i:i+batch_size]\n",
        "\n",
        "            # Get multiple predictions for each input through MC sampling\n",
        "            batch_samples = model.sample_predictions(batch_x, n_samples=n_mc_samples)\n",
        "            all_samples.append(batch_samples.cpu().numpy())\n",
        "\n",
        "    # Concatenate batch results: shape (n_mc_samples, n_samples, 1)\n",
        "    all_predictions = np.concatenate(all_samples, axis=1)\n",
        "    all_predictions = all_predictions.squeeze(axis=2)  # Remove last dimension\n",
        "\n",
        "    # Calculate mean and variance statistics\n",
        "    mean_pred = np.mean(all_predictions, axis=0)\n",
        "\n",
        "    # Decompose uncertainty:\n",
        "    # 1. Total Uncertainty: predictive variance\n",
        "    total_uncertainty = np.var(all_predictions, axis=0)\n",
        "\n",
        "    # 2. Epistemic Uncertainty: variance of means\n",
        "    pred_eu = total_uncertainty.copy()\n",
        "\n",
        "    # 3. Aleatoric Uncertainty: For Bayesian NNs, this is more complex\n",
        "    # We use the expected data noise - mean squared error around mean prediction\n",
        "    pred_au = np.mean(np.square(all_predictions - mean_pred[np.newaxis, :]), axis=0)\n",
        "\n",
        "    # Print samples for comparison\n",
        "    print(\"\\nSample predictions (first 10):\")\n",
        "    print(\"True AU:\", true_au[:10])\n",
        "    print(\"Pred AU:\", pred_au[:10])\n",
        "    print(\"True EU:\", true_eu[:10])\n",
        "    print(\"Pred EU:\", pred_eu[:10])\n",
        "\n",
        "    # Compute metrics\n",
        "    results = {\n",
        "        'AU_AUPRC': average_precision_score(true_au, pred_au),\n",
        "        'AU_AUROC': roc_auc_score(true_au, pred_au),\n",
        "        'EU_AUPRC': average_precision_score(true_eu, pred_eu),\n",
        "        'EU_AUROC': roc_auc_score(true_eu, pred_eu)\n",
        "    }\n",
        "\n",
        "    return results, mean_pred, pred_au, pred_eu\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create dataset with the same parameters as the notebook\n",
        "    n_dim = 2\n",
        "    print(\"Generating dataset...\")\n",
        "    dataset = NDimRoomDataset(30000, n_dim=n_dim, split=\"train\", random_seed=42)\n",
        "    test_dataset = NDimRoomDataset(10000, n_dim=n_dim, split=\"test\", random_seed=42)\n",
        "\n",
        "    # Use a subset of data for faster training\n",
        "    subset_size = 5000\n",
        "    indices = np.random.choice(len(dataset.X), subset_size, replace=False)\n",
        "    X_train = dataset.X[indices]\n",
        "    y_train = dataset.kappa[indices]\n",
        "\n",
        "    # Create a validation set\n",
        "    val_size = 1000\n",
        "    val_indices = np.random.choice(len(test_dataset.X), val_size, replace=False)\n",
        "    X_val = test_dataset.X[val_indices]\n",
        "    y_val = test_dataset.kappa[val_indices]\n",
        "\n",
        "    print(f\"Training on {subset_size} examples, validating on {val_size} examples\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Train the BNN\n",
        "    model = train_bnn(\n",
        "        train_data=(X_train, y_train),\n",
        "        val_data=(X_val, y_val),\n",
        "        input_dim=n_dim,\n",
        "        hidden_dims=[128, 256, 128],\n",
        "        batch_size=64,\n",
        "        epochs=150,\n",
        "        learning_rate=0.001,\n",
        "        beta=1.0,  # KL weight\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Evaluate uncertainty quantification\n",
        "    print(\"Evaluating BNN uncertainty quantification...\")\n",
        "    results, _, _, _ = test_bnn_uncertainty(\n",
        "        model, n_dim=n_dim, n_samples=1000, n_mc_samples=30, device=device\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nBayesian Neural Network Results:\")\n",
        "    print(f\"Aleatoric Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['AU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['AU_AUROC']:.4f}\")\n",
        "    print(f\"Epistemic Uncertainty:\")\n",
        "    print(f\"  AUPRC: {results['EU_AUPRC']:.4f}\")\n",
        "    print(f\"  AUROC: {results['EU_AUROC']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCDZziHzYmtr",
        "outputId": "004d8f3e-c632-4028-f43f-69690b4ba54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset...\n",
            "Training on 5000 examples, validating on 1000 examples\n",
            "Using device: cuda\n",
            "Training Bayesian Neural Network on cuda...\n",
            "Epoch 10/150, Train Loss: 49.7422, Val Loss: 49.3162\n",
            "Epoch 20/150, Train Loss: 41.1316, Val Loss: 40.5895\n",
            "Epoch 30/150, Train Loss: 33.2261, Val Loss: 32.9048\n",
            "Epoch 40/150, Train Loss: 26.2626, Val Loss: 25.9121\n",
            "Epoch 50/150, Train Loss: 20.6402, Val Loss: 20.6483\n",
            "Epoch 60/150, Train Loss: 16.7646, Val Loss: 16.5626\n",
            "Epoch 70/150, Train Loss: 14.0563, Val Loss: 14.0456\n",
            "Epoch 80/150, Train Loss: 12.2978, Val Loss: 12.1495\n",
            "Epoch 90/150, Train Loss: 10.9778, Val Loss: 10.8362\n",
            "Epoch 100/150, Train Loss: 9.8545, Val Loss: 9.8719\n",
            "Epoch 110/150, Train Loss: 9.0095, Val Loss: 8.8819\n",
            "Epoch 120/150, Train Loss: 8.4620, Val Loss: 8.4864\n",
            "Epoch 130/150, Train Loss: 7.9327, Val Loss: 7.6859\n",
            "Epoch 140/150, Train Loss: 7.3697, Val Loss: 7.3141\n",
            "Epoch 150/150, Train Loss: 7.0280, Val Loss: 6.9745\n",
            "Training complete!\n",
            "Evaluating BNN uncertainty quantification...\n",
            "\n",
            "Sample Distribution:\n",
            "No Uncertainty: 0.325\n",
            "Only AU: 0.347\n",
            "EU: 0.328\n",
            "\n",
            "Performing Monte Carlo sampling from the BNN...\n",
            "\n",
            "Sample predictions (first 10):\n",
            "True AU: [0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Pred AU: [0.00181141 0.00209336 0.00279874 0.0030505  0.00190468 0.00807838\n",
            " 0.00107959 0.00213005 0.02773866 0.00012497]\n",
            "True EU: [1. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Pred EU: [0.00181141 0.00209336 0.00279874 0.0030505  0.00190468 0.00807838\n",
            " 0.00107959 0.00213005 0.02773866 0.00012497]\n",
            "\n",
            "Bayesian Neural Network Results:\n",
            "Aleatoric Uncertainty:\n",
            "  AUPRC: 0.3438\n",
            "  AUROC: 0.4722\n",
            "Epistemic Uncertainty:\n",
            "  AUPRC: 0.6169\n",
            "  AUROC: 0.8436\n"
          ]
        }
      ]
    }
  ]
}